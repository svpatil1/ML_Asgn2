{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective Functions: A Simple Example with Matrix Factorisation.\n",
    "\n",
    "### 6th October 2015 Neil D. Lawrence\n",
    "\n",
    "### Modified by Mauricio A Álvarez, 1st October 2018\n",
    "\n",
    "In last week's class we saw how we could load in a data set to pandas and use it for some simple data processing. We computed various probabilities on the data and I encouraged you to think about what sort of probabilities you need for prediction. This week we are going to take a slightly different tack. \n",
    "\n",
    "Broadly speaking there are two dominating approaches to machine learning problems. We started to consider the first approach last week: constructing models based on defining the relationship between variables using probabilities. This week we will consider the second approach: which involves defining an *objective function* and optimizing it. \n",
    "\n",
    "What do we mean by an objective function? An objective function could be an *error function*, a *cost function* or a *benefit* function. In evolutionary computing they are called *fitness* functions. But the idea is always the same. We write down a mathematical equation which is then optimized to do the learning. The equation should be a function of the *data* and our model *parameters*. We have a choice when optimizing, either minimize or maximize. To avoid confusion, in the optimization field, we always choose to minimize the function. If we have a function that we would like to maximize, we simply choose to minimize the negative of that function. \n",
    "\n",
    "So for this lab session, we are going to ignore probabilities, but don't worry, they will return! \n",
    "\n",
    "This week we are going to try and build a simple movie recommender system using an objective function. To do this, the first thing I'd like you to do is to install some software we've written for sharing information across google documents.\n",
    "\n",
    "## Open Data Science Software\n",
    "\n",
    "In Sheffield we have written a suite of software tools for 'Open Data Science'. Open data science is an approach to sharing code, models and data that should make it easier for companies, health professionals and scientists to gain access to data science techniques. For some background on open data science you can read [this blog post](http://inverseprobability.com/2014/07/01/open-data-science/). The first thing we will do this week is to download that suite of software. \n",
    "\n",
    "The software can be installed using\n",
    "\n",
    "```python\n",
    "pip install pods\n",
    "```\n",
    "\n",
    "from the command prompt where you can access your python installation.\n",
    "\n",
    "\n",
    "## Download the MovieLens 100k Data\n",
    "\n",
    "We are going to download the [MovieLens 100k](http://files.grouplens.org/datasets/movielens/ml-latest-small-README.html) Data. This is a public dataset that contains 100,000 ratings and 3,600 tag applications applied to 9,000 movies by 600 users. When you use a data set that someone has prepared you should always reference the data source to acknowledge the work that's been placed in. This particular dataset was collected by the [Grouplens Research group](https://grouplens.org/),  at the University of Minnesota. For example, if you were to use this dataset for writing a paper, the authors ask you that you acknowledge their work by citing the following paper:\n",
    "\n",
    "F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5 (4):1-19 [https://doi.org/10.1145/2827872](https://doi.org/10.1145/2827872)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading  http://files.grouplens.org/datasets/movielens/ml-latest-small.zip -> ./ml-latest-small.zip\n",
      "[==============================]   0.933/0.933MB                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n"
     ]
    }
   ],
   "source": [
    "import pods\n",
    "import zipfile\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pods.util.download_url(\"http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\")\n",
    "zip_console = zipfile.ZipFile('ml-latest-small.zip', 'r')\n",
    "for name in zip_console.namelist():\n",
    "           zip_console.extract(name, './')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Question 1\n",
    "\n",
    "Data ethics. If you find data available on the internet, can you simply use it without consequence? If you are given data by a fellow researcher can you publish that data on line? \n",
    "\n",
    "*5 marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 Answer\n",
    "\n",
    "Write your answer to the question in this box.\n",
    "\n",
    "Under traditional ethics framework, I should not simply use the data available on internet without knowing its consequence. Every human being value their privacy and protection of personal data. But recent advances in the technologies have threaten privacy and the amount of control over the use of personal data. One should always follow the ethical guidelines before using any data on the internet.\n",
    "\n",
    "I cannot publish any data online given by the researchers. Moreover, it is also incorrect that the researcher gives the data without considering data ethics. He should follow some data ethics framework and guidelines like Informed consent, Be aware of relevant legislation and codes of practice etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender Systems\n",
    "\n",
    "A recommender system aims to make suggestions for items (films, books, other commercial products) given what it knows about users' tastes. The recommendation engine needs to represent the *taste* of all the users and the *characteristics* of each object. \n",
    "\n",
    "A common way for organizing objects is to place related objects spatially close together. For example in a library we try and put books that are on related topics near to each other on the shelves. One system for doing this is known as [Dewey Decimal Classification](http://en.wikipedia.org/wiki/Dewey_Decimal_Classification). In the Dewey Decimal Classification system (which dates from 1876) each subject is given a number (in fact it's a decimal number). For example, the field of Natural Sciences and Mathematics is given numbers which start with 500. Subjects based on Computer Science are given numbers which start 004 and works on the 'mathematical principles' of Computer science are given the series 004.0151 (which we might store as 4.0151 on a Computer). Whilst it's a classification system, the books in the library are typically laid out in the same order as the numbers, so we might expect that neighbouring numbers represent books that are related in subject. That seems to be exactly what we want when also representing films. Could we somehow represent each film's subject according to a number? In a similar way we could then imagine representing users with a list of numbers that represent things that each user is interested in.\n",
    "\n",
    "Actually a one dimensional representation of a subject can be very awkward. To see this, let's have a look at the Dewey Decimal Classification numbers for the 900s, which is listed as 'History and Geography'. We will focus on subjects in the 940s which can be found in this list from [Wikipedia](https://en.wikipedia.org/wiki/List_of_Dewey_Decimal_classes#Class_900_%E2%80%93_History_&_geography). Whilst the ordering for places is somewhat sensible, it is also rather arbitrary. In the 940s we have Europe listed from 940-949, Asia listed from 950-959 and Africa listed from 960-969. Whilst it's true that Asia borders Europe, Africa is also very close, and the history of the Roman Empire spreads into [Carthage](http://en.wikipedia.org/wiki/Carthage) and later on Egypt. This image from Wikipedia shows a map of the Cathaginian Empire which fell after fighting with Rome. \n",
    "\n",
    "\n",
    "<a title=\"By Javierfv1212 [Public domain], from Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Carthaginianempire.PNG\"><img width=\"512\" alt=\"Carthaginianempire\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/9b/Carthaginianempire.PNG/512px-Carthaginianempire.PNG\"></a>\n",
    "\n",
    "We now need to make a decision about whether Roman Histories are European or African, ideally we'd like them to be somewhere between the two, but we can't place them there in the Dewey Decimal system because between Europe and Africa is Asia, which has less to do with the Roman Empire than either Europe or Africa. Of course the fact that we've used a map provides a clue as to what to do next. Libraries are actually laid out on floors, so what if we were to use the spatial lay out to organise the sujbects of the books in two dimensions. Books on Geography could be laid out according to where in the world they are referring to. \n",
    "\n",
    "Such complexities are very hard to encapsulate in one number, but inspired by the map examples we can start considering how we might lay out films in two dimensions. Similarly, we can consider laying out a map of people's interests. If the two maps correspond to one another, the map of people could reflect where they might want to live in 'subject space'. We can think of representing people's tastes as where they might best like to sit in the library to access easily the books they are most interested in.\n",
    "\n",
    "\n",
    "## Inner Products for Representing Similarity\n",
    "\n",
    "Ideas like the above are good for gaining intuitions about what we might want, but the one of the skills of data science is representing those ideas mathematically. Mathematical abstraction of a problem is one of the key ways in which we've been able to progress as a society. Understanding planetary motions, as well as those of the smallest molecule (to quote Laplace's [Philosophical Essay on Probabilities](http://books.google.co.uk/books?id=1YQPAAAAQAAJ&printsec=frontcover&source=gbs_ge_summary_r&cad=0#v=onepage&q&f=false)) needed to be done mathematically. The right mathematical model in machine learning can be slightly more elusive, because constructing it is a two stage process. \n",
    "\n",
    "1. We have to determine the right intuition for the system we want to represent. Notions such as 'subject' and 'interest' are not mathematically well defined, and even when we create a new interpretation of what they might mean, each interpretation may have its own weaknesses. \n",
    "\n",
    "2. Once we have our interpretation we can attempt to mathematically formalize it. In our library interpretation, that's what we need to do next. \n",
    "\n",
    "### The Library on an Infinite Plane\n",
    "\n",
    "Let's imagine a library which stores all the items  we are interested in, not just books, but films and shopping items too. Such a library is likely to be very large, so we'll create it on an infinite two dimensional plane. This means we can use all the real numbers to represent the location of each item on the plane. For a two dimensional plane, we need to store the locations in a vector of numbers: we can decide that the $j$th item's location in the library is given by \n",
    "$$\n",
    "\\mathbf{v}_j = \\begin{bmatrix} v_{j,1} \\\\ v_{j,2}\\end{bmatrix},\n",
    "$$\n",
    "where $v_{j,1}$ represents the $j$th item's location in the East-West direction (or the $x$-axis) and $v_{j,2}$ represents the $j$th item's location in the North-South direction (or the $y$-axis). Now we need to specify the location where each user sits so that all the items that interest them are nearby: we can also represent the $i$th user's location with a vector \n",
    "$$\n",
    "\\mathbf{u}_i = \\begin{bmatrix} u_{i,1} \\\\ u_{i,2}\\end{bmatrix}.\n",
    "$$\n",
    "Finally, we need some way of recording a given user's affinity for a given item. This affinity might be the rating that the user gives the film. We can use $y_{i,j}$ to represent user $i$'s affinity for item $j$. \n",
    "\n",
    "For our film example we might imagine wanting to order films in a few ways. We could imagine organising films in the North-South direction as to how romantic they are. We could place the more romantic films further North and the less romantic films further South. For the East-West direction we could imagine ordering them according to how historic they are: we can imagine placing science fiction films to the East and historical drama to the West. In this case, fans of historical romances would be based in the North-West location, whilst fans of Science Fiction Action films might be located in the South-East (if we assume that 'Action' is the opposite of 'Romance', which is not necessarily the case). How do we lay out all these films? Have we got the right axes? In machine learning the answer is to 'let the data speak'. Use the data to try and obtain such a lay out. To do this we first need to obtain the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the Data\n",
    "\n",
    "As mentioned before, the MovieLens dataset that we'll use has 100,000 ratings to 9,000 movies by 600 users. For now, we will only work with a subset of the dataset. In particular, we will randomly chose a particular number of users and extract the movies and ratings that the users gave to those movies. Read the code below and understand what it is doing.\n",
    "\n",
    "**Before you run the code**, notice that `YourStudentID` in the first line is a variable that will specify the seed for the random number generator that will select a particular set of `nUsersInExample` users. Change the number that has been assigned by default to `YourStudentID` to the last three digits of your UCard number. All of you will have a different subset of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0    1    2   3    4    5    6    7    8    9\n",
      "0    NaN  NaN  NaN NaN  5.0  4.5  4.0  5.0  2.0  4.0\n",
      "1    NaN  NaN  NaN NaN  NaN  NaN  NaN  NaN  NaN  3.5\n",
      "2    NaN  NaN  NaN NaN  NaN  NaN  4.0  NaN  NaN  NaN\n",
      "3    NaN  NaN  NaN NaN  3.0  NaN  NaN  NaN  NaN  NaN\n",
      "4    4.0  NaN  NaN NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "5    NaN  NaN  NaN NaN  4.0  NaN  3.0  NaN  NaN  NaN\n",
      "6    NaN  NaN  NaN NaN  NaN  NaN  3.0  NaN  NaN  NaN\n",
      "7    NaN  NaN  NaN NaN  4.0  NaN  NaN  3.0  NaN  3.5\n",
      "8    NaN  NaN  NaN NaN  NaN  NaN  NaN  NaN  NaN  3.5\n",
      "9    NaN  NaN  NaN NaN  3.0  NaN  NaN  NaN  NaN  NaN\n",
      "10   NaN  NaN  NaN NaN  NaN  NaN  4.0  NaN  NaN  NaN\n",
      "11   NaN  NaN  NaN NaN  NaN  NaN  3.0  NaN  NaN  NaN\n",
      "12   NaN  NaN  NaN NaN  2.0  NaN  NaN  NaN  NaN  4.0\n",
      "13   4.0  NaN  NaN NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "14   3.5  NaN  NaN NaN  NaN  NaN  4.0  4.0  NaN  4.0\n",
      "15   NaN  NaN  NaN NaN  NaN  NaN  5.0  NaN  NaN  3.5\n",
      "16   NaN  NaN  NaN NaN  NaN  NaN  NaN  3.0  NaN  3.0\n",
      "17   NaN  2.0  NaN NaN  NaN  NaN  4.0  NaN  NaN  3.5\n",
      "18   4.0  NaN  5.0 NaN  NaN  4.0  NaN  5.0  NaN  NaN\n",
      "19   NaN  NaN  NaN NaN  NaN  NaN  4.0  NaN  NaN  NaN\n",
      "20   3.0  NaN  NaN NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "21   NaN  NaN  NaN NaN  4.0  NaN  NaN  NaN  NaN  NaN\n",
      "22   NaN  NaN  NaN NaN  NaN  NaN  3.0  NaN  NaN  NaN\n",
      "23   NaN  NaN  NaN NaN  NaN  NaN  3.0  NaN  NaN  2.5\n",
      "24   NaN  NaN  NaN NaN  NaN  NaN  4.0  NaN  NaN  NaN\n",
      "25   NaN  NaN  5.0 NaN  NaN  4.0  5.0  5.0  NaN  3.5\n",
      "26   NaN  NaN  NaN NaN  NaN  NaN  NaN  4.0  NaN  3.0\n",
      "27   NaN  NaN  NaN NaN  NaN  NaN  4.0  NaN  NaN  NaN\n",
      "28   NaN  NaN  NaN NaN  NaN  NaN  3.0  NaN  NaN  NaN\n",
      "29   NaN  NaN  NaN NaN  4.0  NaN  4.0  NaN  NaN  2.5\n",
      "..   ...  ...  ...  ..  ...  ...  ...  ...  ...  ...\n",
      "560  NaN  NaN  NaN NaN  NaN  4.0  NaN  NaN  NaN  NaN\n",
      "561  NaN  NaN  NaN NaN  NaN  3.5  NaN  NaN  NaN  NaN\n",
      "562  NaN  NaN  NaN NaN  NaN  5.0  NaN  NaN  NaN  NaN\n",
      "563  NaN  NaN  NaN NaN  NaN  3.5  NaN  NaN  NaN  NaN\n",
      "564  NaN  NaN  NaN NaN  NaN  4.5  NaN  NaN  NaN  NaN\n",
      "565  NaN  NaN  NaN NaN  NaN  4.5  NaN  NaN  NaN  NaN\n",
      "566  NaN  NaN  5.0 NaN  NaN  3.0  NaN  NaN  NaN  NaN\n",
      "567  NaN  NaN  NaN NaN  NaN  4.5  NaN  NaN  NaN  NaN\n",
      "568  NaN  NaN  NaN NaN  NaN  4.0  NaN  NaN  NaN  NaN\n",
      "569  NaN  NaN  NaN NaN  NaN  4.0  NaN  NaN  NaN  NaN\n",
      "570  NaN  NaN  NaN NaN  NaN  4.5  NaN  NaN  NaN  NaN\n",
      "571  NaN  NaN  NaN NaN  NaN  3.5  NaN  NaN  NaN  NaN\n",
      "572  NaN  NaN  NaN NaN  NaN  4.0  NaN  NaN  NaN  NaN\n",
      "573  NaN  NaN  NaN NaN  NaN  3.5  NaN  NaN  NaN  NaN\n",
      "574  NaN  NaN  NaN NaN  NaN  NaN  NaN  NaN  0.5  NaN\n",
      "575  NaN  NaN  NaN NaN  NaN  4.0  NaN  NaN  NaN  NaN\n",
      "576  NaN  NaN  NaN NaN  NaN  3.0  NaN  NaN  NaN  NaN\n",
      "577  NaN  NaN  NaN NaN  NaN  5.0  NaN  NaN  NaN  NaN\n",
      "578  NaN  NaN  NaN NaN  NaN  4.0  NaN  NaN  NaN  NaN\n",
      "579  NaN  NaN  NaN NaN  NaN  5.0  NaN  NaN  NaN  NaN\n",
      "580  NaN  NaN  NaN NaN  NaN  4.5  NaN  NaN  NaN  NaN\n",
      "581  NaN  NaN  NaN NaN  NaN  NaN  NaN  NaN  4.0  NaN\n",
      "582  NaN  NaN  NaN NaN  NaN  4.0  NaN  NaN  NaN  NaN\n",
      "583  NaN  NaN  NaN NaN  NaN  4.0  NaN  NaN  NaN  NaN\n",
      "584  NaN  NaN  NaN NaN  NaN  3.5  NaN  NaN  NaN  NaN\n",
      "585  NaN  NaN  4.5 NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "586  NaN  NaN  2.0 NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "587  NaN  NaN  4.5 NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "588  NaN  NaN  3.5 NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "589  NaN  NaN  3.5 NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "\n",
      "[590 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "YourStudentID = 356  # Include here the last three digits of your UCard number\n",
    "nUsersInExample = 10 # The maximum number of Users we're going to analyse at one time\n",
    "\n",
    "ratings = pd.read_csv(\"./ml-latest-small/ratings.csv\") \n",
    "\"\"\"\n",
    "ratings is a DataFrame with four columns: userId, movieId, rating and tags. We\n",
    "first want to identify how many unique users there are. We can use the unique \n",
    "method in pandas\n",
    "\"\"\"\n",
    "indexes_unique_users = ratings['userId'].unique()\n",
    "n_users = indexes_unique_users.shape[0]   #shape is used to get the current shape of the array\n",
    "#print(n_users)\n",
    "\"\"\" \n",
    "We randomly select 'nUsers' users with their ratings. We first fix the seed\n",
    "of the random generator to make sure that we always get the same 'nUsers'\n",
    "\"\"\"\n",
    "np.random.seed(YourStudentID)\n",
    "indexes_users = np.random.permutation(n_users)  #Randomly permute a sequence, or return a permuted range.\n",
    "my_batch_users = indexes_users[0:nUsersInExample]\n",
    "#print(my_batch_users)\n",
    "\"\"\"\n",
    "We will use now the list of 'my_batch_users' to create a matrix Y. \n",
    "\"\"\"\n",
    "# We need to make a list of the movies that these users have watched\n",
    "list_movies_each_user = [[] for _ in range(nUsersInExample)]\n",
    "list_ratings_each_user = [[] for _ in range(nUsersInExample)]\n",
    "# Movies\n",
    "list_movies = ratings['movieId'][ratings['userId'] == my_batch_users[0]].values\n",
    "list_movies_each_user[0] = list_movies \n",
    "#print(list_movies)\n",
    "# Ratings                      \n",
    "list_ratings = ratings['rating'][ratings['userId'] == my_batch_users[0]].values\n",
    "list_ratings_each_user[0] = list_ratings\n",
    "#print(list_ratings)\n",
    "# Users\n",
    "n_each_user = list_movies.shape[0]\n",
    "#print(n_each_user)\n",
    "list_users = my_batch_users[0]*np.ones((1, n_each_user)) #np.ones > Return a new array of given shape and type, filled with ones\n",
    "\n",
    "\n",
    "for i in range(1, nUsersInExample):\n",
    "    # Movies\n",
    "    local_list_per_user_movies = ratings['movieId'][ratings['userId'] == my_batch_users[i]].values\n",
    "    list_movies_each_user[i] = local_list_per_user_movies\n",
    "    list_movies = np.append(list_movies,local_list_per_user_movies)\n",
    "    # Ratings                                 \n",
    "    local_list_per_user_ratings = ratings['rating'][ratings['userId'] == my_batch_users[i]].values\n",
    "    list_ratings_each_user[i] = local_list_per_user_ratings\n",
    "    list_ratings = np.append(list_ratings, local_list_per_user_ratings)  \n",
    "    # Users                                   \n",
    "    n_each_user = local_list_per_user_movies.shape[0]                                                                               \n",
    "    local_rep_user =  my_batch_users[i]*np.ones((1, n_each_user))    \n",
    "    list_users = np.append(list_users, local_rep_user)\n",
    "\n",
    "# Let us first see how many unique movies have been rated\n",
    "indexes_unique_movies = np.unique(list_movies)\n",
    "n_movies = indexes_unique_movies.shape[0]\n",
    "#print(n_movies)\n",
    "# As it is expected no all users have rated all movies. We will build a matrix Y \n",
    "# with NaN inputs and fill according to the data for each user \n",
    "temp = np.empty((n_movies,nUsersInExample,))\n",
    "temp[:] = np.nan\n",
    "Y_with_NaNs = pd.DataFrame(temp)\n",
    "for i in range(nUsersInExample):\n",
    " local_movies = list_movies_each_user[i]\n",
    " ixs = np.in1d(indexes_unique_movies, local_movies)\n",
    " Y_with_NaNs.loc[ixs, i] = list_ratings_each_user[i]\n",
    "print(Y_with_NaNs)\n",
    "Y_with_NaNs.index = indexes_unique_movies.tolist()\n",
    "Y_with_NaNs.columns = my_batch_users.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = indexes_unique_users.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Question 2\n",
    "\n",
    "Have a look at the matrix `Y_with_NaNs`. The movies data is now in a data frame which contains one column for each user rating the movie. There are some entries that contain 'NaN'. What does the 'NaN' mean in this context?\n",
    "\n",
    "*5 marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer Question 2\n",
    "\n",
    "Write your answer to the question in this box.\n",
    "\n",
    "Here we have generated random 10 users ( i.e. [ 23  75  296  547  31  562  179  46  193  570 ] ) considering the YourStudentID = 356. Using the following information we have calculated the total number of unique movies ( i.e. 590 ) that are rated by all of these 10 users. But as every movie is not rated by every user , we have feed that user-movie pair in the data frame with NaN input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will convert our data structure into a form that is appropriate for processing. We will convert the `Y_with_NaNs` dataframe into a new dataframe which contains the user, the movie, and the rating using the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     movies   ratings  ratingsorig  users\n",
      "0         6  0.238318          4.0   23.0\n",
      "1        29  0.238318          4.0   23.0\n",
      "2        32 -0.261682          3.5   23.0\n",
      "3        50  0.238318          4.0   23.0\n",
      "4        58 -0.761682          3.0   23.0\n",
      "5       175 -0.761682          3.0   23.0\n",
      "6       272  0.238318          4.0   23.0\n",
      "7       293  0.238318          4.0   23.0\n",
      "8       296 -0.261682          3.5   23.0\n",
      "9       300 -0.761682          3.0   23.0\n",
      "10      334  0.238318          4.0   23.0\n",
      "11      431 -0.761682          3.0   23.0\n",
      "12      454 -1.261682          2.5   23.0\n",
      "13      541  1.238318          5.0   23.0\n",
      "14      608 -0.261682          3.5   23.0\n",
      "15      741  0.238318          4.0   23.0\n",
      "16      750 -0.761682          3.0   23.0\n",
      "17      778  0.238318          4.0   23.0\n",
      "18      858 -0.261682          3.5   23.0\n",
      "19      866 -0.761682          3.0   23.0\n",
      "20      904 -0.261682          3.5   23.0\n",
      "21      912 -0.261682          3.5   23.0\n",
      "22      913  0.238318          4.0   23.0\n",
      "23      923  0.238318          4.0   23.0\n",
      "24      924  0.238318          4.0   23.0\n",
      "25     1036 -0.761682          3.0   23.0\n",
      "26     1050 -0.261682          3.5   23.0\n",
      "27     1080 -0.261682          3.5   23.0\n",
      "28     1089  0.238318          4.0   23.0\n",
      "29     1136 -0.761682          3.0   23.0\n",
      "..      ...       ...          ...    ...\n",
      "826    4226  0.238318          4.0  570.0\n",
      "827    4306  0.238318          4.0  570.0\n",
      "828    4878  0.738318          4.5  570.0\n",
      "829    4886 -0.261682          3.5  570.0\n",
      "830    4993  0.238318          4.0  570.0\n",
      "831    5349  0.238318          4.0  570.0\n",
      "832    5445  0.738318          4.5  570.0\n",
      "833    5902  0.238318          4.0  570.0\n",
      "834    5989 -0.261682          3.5  570.0\n",
      "835    6323  0.238318          4.0  570.0\n",
      "836    6333  0.238318          4.0  570.0\n",
      "837    6539 -0.261682          3.5  570.0\n",
      "838    6942  0.738318          4.5  570.0\n",
      "839    6953  0.238318          4.0  570.0\n",
      "840    7153 -0.261682          3.5  570.0\n",
      "841    7254  0.238318          4.0  570.0\n",
      "842    7361  0.238318          4.0  570.0\n",
      "843    8636 -0.261682          3.5  570.0\n",
      "844   33794 -0.261682          3.5  570.0\n",
      "845   45722  0.238318          4.0  570.0\n",
      "846   47099  0.238318          4.0  570.0\n",
      "847   48516 -0.261682          3.5  570.0\n",
      "848   48738 -0.261682          3.5  570.0\n",
      "849   48774  0.238318          4.0  570.0\n",
      "850   48780 -0.261682          3.5  570.0\n",
      "851   49272  0.238318          4.0  570.0\n",
      "852   49530 -0.261682          3.5  570.0\n",
      "853   52328 -0.261682          3.5  570.0\n",
      "854   53000 -0.761682          3.0  570.0\n",
      "855   53125 -0.761682          3.0  570.0\n",
      "\n",
      "[856 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "p_list_ratings = np.concatenate(list_ratings_each_user).ravel()  # np.ravel > A 1-D array, containing the elements of the input, is returned\n",
    "#print(list_ratings_each_user)\n",
    "#print(type(p_list_ratings))\n",
    "p_list_ratings_original = p_list_ratings.tolist()\n",
    "mean_ratings_train = np.mean(p_list_ratings)  # returns average of array elements\n",
    "#print(mean_ratings_train)\n",
    "p_list_ratings =  p_list_ratings - mean_ratings_train # remove the mean\n",
    "p_list_movies = np.concatenate(list_movies_each_user).ravel().tolist()\n",
    "p_list_users = list_users.tolist()\n",
    "Y = pd.DataFrame({'users': p_list_users, 'movies': p_list_movies, 'ratingsorig': p_list_ratings_original,'ratings':p_list_ratings.tolist()})\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Question 3\n",
    "\n",
    "The dataframes `Y_with_NaNs` and `Y` contain the same information but organised in a different way. Explain what is the difference. We have also included two columns for ratings in dataframe `Y`, `ratingsorig` and `ratings`. Explain\n",
    "the difference. \n",
    "\n",
    "*10 marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3 Answer\n",
    "\n",
    "Write your answer to the question in this box.\n",
    "\n",
    "The dataframes Y_with_NaNs and Y contains same information. But the dataframe Y_with_NaNs contains the information that is not significant or can be omitted. Y_with_NaNs dataframe also contains the data for the user-movie pair where the user has not rated for that particular movie and is indicated with NaN.\n",
    "\n",
    "Whereas dataframe Y contains only the user-movie information that is significant. It contains the information only for the movies that are rated by each user. It also includes additional two columns as ratingsorig and ratings.\n",
    "\n",
    "The ratingsorig are the original ratings that are given by the users. These ratings are given out of 10. Whereas values in ratings columns gives data with new mean set to zero. This is called zero-centering. Thus the mean lies on zero. This allows different data sets to be compared and allows to analyze the data that how it varies about the mean. This further can be used for normalizing and standardizing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Similarity\n",
    "\n",
    "We now need a measure for determining the similarity between the item and the user: how close the user is sitting to the item in the rooom if you like. We are going to use the inner product between the vector representing the item and the vector representing the user. \n",
    "\n",
    "An inner product (or [dot product](http://en.wikipedia.org/wiki/Dot_product)) between two vectors $\\mathbf{a}$ and $\\mathbf{b}$ is written as $\\mathbf{a}\\cdot\\mathbf{b}$. Or in vector notation we sometimes write it as $\\mathbf{a}^\\top\\mathbf{b}$. An inner product is simply the sume of the products of each element of the vector,\n",
    "$$\n",
    "\\mathbf{a}^\\top\\mathbf{b} = \\sum_{i} a_i b_i\n",
    "$$\n",
    "The inner product can be seen as a measure of similarity. The inner product gives us the cosine of the angle between the two vectors multiplied by their length. The smaller the angle between two vectors the larger the inner product. \n",
    "$$\n",
    "\\mathbf{a}^\\top\\mathbf{b} = |\\mathbf{a}||\\mathbf{b}| \\cos(\\theta)\n",
    "$$\n",
    "where $\\theta$ is the angle between two vectors and $|\\mathbf{a}|$ and $|\\mathbf{b}|$ are the respective lengths of the two vectors.\n",
    "\n",
    "Since we want each user to be sitting near each item, then we want the inner product to be large for any two items which are rated highly by that user. We can do this by trying to force the inner product $\\mathbf{u}_i^\\top\\mathbf{v}_j$ to be similar to the rating given by the user, $y_{i,j}$. To ensure this we will use a least squares objective function for all user ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function\n",
    "\n",
    "The error function (or objective function, or cost function) we will choose is known as 'sum of squares', we will aim to minimize the sum of squared squared error between the inner product of $\\mathbf{u}_i$ and $\\mathbf{v}_i$ and the observed score for the user/item pairing, given by $y_{i, j}$. \n",
    "\n",
    "The total objective function can be written as\n",
    "$$\n",
    "E(\\mathbf{U}, \\mathbf{V}) = \\sum_{i,j} s_{i,j} (y_{i,j} - \\mathbf{u}_i^\\top \\mathbf{v}_j)^2\n",
    "$$\n",
    "where $s_{i,j}$ is an *indicator* variable that is 1 if user $i$ has rated item $j$ and is zero otherwise. Here $\\mathbf{U}$ is the matrix made up of all the vectors $\\mathbf{u}$,\n",
    "$$\n",
    "\\mathbf{U} = \\begin{bmatrix} \\mathbf{u}_1 \\dots \\mathbf{u}_n\\end{bmatrix}^\\top\n",
    "$$\n",
    "where we note that $i$th *row* of $\\mathbf{U}$ contains the vector associated with the $i$th user and $n$ is the total number of users. This form of matrix is known as a *design matrix*. Similarly, we define the matrix\n",
    "$$\n",
    "\\mathbf{V} = \\begin{bmatrix} \\mathbf{v}_1 \\dots \\mathbf{v}_m\\end{bmatrix}^\\top\n",
    "$$\n",
    "where again the $j$th row of $\\mathbf{V}$ contains the vector associated with the $j$th item and $m$ is the total number of items in the data set.\n",
    "\n",
    "## Objective Optimization\n",
    "\n",
    "The idea is to mimimize this objective. A standard, simple, technique for minimizing an objective is *gradient descent* or *steepest descent*. In gradient descent we simply choose to update each parameter in the model by subtracting a multiple of the objective function's gradient with respect to the parameters. So for a parameter $u_{i,j}$ from the matrix $\\mathbf{U}$ we would have an update as follows:\n",
    "$$\n",
    "u_{k,\\ell} \\leftarrow u_{k,\\ell} - \\eta \\frac{\\text{d} E(\\mathbf{U}, \\mathbf{V})}{\\text{d}u_{k,\\ell}} \n",
    "$$\n",
    "where $\\eta$ (which is pronounced *eta* in English) is a Greek letter representing the *learning rate*.  \n",
    "\n",
    "We can compute the gradient of the objective function with respect to $u_{k,\\ell}$ as\n",
    "$$\n",
    "\\frac{\\text{d}E(\\mathbf{U}, \\mathbf{V})}{\\text{d}u_{k,\\ell}} = -2 \\sum_j s_{k,j}v_{j,\\ell}(y_{k, j} - \\mathbf{u}_k^\\top\\mathbf{v}_{j}). \n",
    "$$\n",
    "Similarly each parameter $v_{i,j}$ needs to be updated according to its gradient. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Question 4\n",
    "\n",
    "What is the gradient of the objective function with respect to $v_{k, \\ell}$? Write your answer in the box below, and explain which differentiation techniques you used to get there. You will be expected to justify your answer in class by oral questioning. \n",
    "\n",
    "*15 marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 Answer\n",
    "\n",
    "Write your answer to the question in this box.\n",
    "\n",
    "The diffrentiation techniques used to get to that answer were,\n",
    "firstly the power rule and secondly the chain rule of diffrentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 Code Answer\n",
    "\n",
    "$$\n",
    "\\frac{\\text{d}E(\\mathbf{U}, \\mathbf{V})}{\\text{d}v_{k,\\ell}} = -2 \\sum_i s_{k,i}u_{i,\\ell}^\\top\\mathbf(y_{i, k} - \\mathbf{u}i^\\top\\mathbf{v}{k}). \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steepest Descent Algorithm\n",
    "\n",
    "In the steepest descent algorithm we aim to minimize the objective function by subtacting the gradient of the objective function from the parameters. \n",
    "\n",
    "### Initialisation\n",
    "\n",
    "To start with though, we need initial values for the matrix $\\mathbf{U}$ and the matrix $\\mathbf{V}$. Let's create them as `pandas` data frames and initialise them randomly with small values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0         1\n",
      "23  -0.000033 -0.000564\n",
      "75  -0.000276  0.001285\n",
      "296  0.000101  0.000686\n",
      "547 -0.000138  0.000530\n",
      "31  -0.000764 -0.000764\n",
      "562 -0.000894  0.001008\n",
      "179 -0.002097  0.000366\n",
      "46   0.002717  0.000540\n",
      "193  0.000680 -0.001171\n",
      "570  0.001757  0.001450\n",
      "               0         1\n",
      "1      -0.001344 -0.001755\n",
      "2       0.000033 -0.001274\n",
      "3      -0.000861  0.000757\n",
      "5      -0.000384  0.001156\n",
      "6       0.002014 -0.000157\n",
      "7       0.000951 -0.000835\n",
      "9      -0.001061  0.000655\n",
      "10     -0.001099 -0.000469\n",
      "11      0.000362 -0.000995\n",
      "17     -0.001054  0.000073\n",
      "18      0.001715  0.002288\n",
      "19      0.000643  0.001242\n",
      "25      0.000481  0.000491\n",
      "29     -0.000240  0.000218\n",
      "32     -0.001697 -0.000299\n",
      "34      0.000355 -0.000045\n",
      "39     -0.000858  0.000049\n",
      "47     -0.001212 -0.000265\n",
      "50      0.001166  0.000615\n",
      "52     -0.001154 -0.000615\n",
      "58     -0.001521  0.001002\n",
      "62     -0.000398 -0.001609\n",
      "65     -0.001139 -0.000520\n",
      "95     -0.000104 -0.001113\n",
      "104    -0.001123  0.000658\n",
      "110     0.000227  0.000043\n",
      "111     0.000438 -0.000505\n",
      "112    -0.000416 -0.000818\n",
      "113    -0.001438 -0.000434\n",
      "141    -0.000186  0.000252\n",
      "...          ...       ...\n",
      "68157   0.000417  0.002032\n",
      "70336  -0.000736  0.001802\n",
      "71899   0.001220 -0.000360\n",
      "73211  -0.000427 -0.001414\n",
      "74324   0.001673 -0.001175\n",
      "78499  -0.000448 -0.000015\n",
      "79132   0.000494  0.000428\n",
      "79695  -0.000624  0.001713\n",
      "80831  -0.000440  0.000701\n",
      "80860  -0.000571  0.000752\n",
      "82041   0.000553 -0.001342\n",
      "82167  -0.001391  0.001453\n",
      "83134  -0.000939 -0.000300\n",
      "84374  -0.001279  0.000954\n",
      "86347  -0.000226  0.000768\n",
      "87306  -0.001019  0.000263\n",
      "88129   0.002436 -0.000578\n",
      "88163   0.000113  0.000545\n",
      "91485  -0.000262  0.000495\n",
      "93840  -0.000839 -0.001514\n",
      "97921  -0.001253 -0.001601\n",
      "97938  -0.002263 -0.002767\n",
      "99007  -0.001039  0.000204\n",
      "99191  -0.001277 -0.002259\n",
      "100306  0.000725  0.002207\n",
      "160848 -0.000527  0.000474\n",
      "166528  0.000422 -0.000229\n",
      "169034 -0.000521  0.001263\n",
      "171765 -0.000765  0.001007\n",
      "180031  0.000929 -0.002005\n",
      "\n",
      "[590 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "q = 2 # the dimension of our map of the 'library'\n",
    "learn_rate = 0.02\n",
    "U = pd.DataFrame(np.random.normal(size=(nUsersInExample, q))*0.001, index=my_batch_users)\n",
    "print(U)\n",
    "V = pd.DataFrame(np.random.normal(size=(n_movies, q))*0.001, index=indexes_unique_movies)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the initial values set, we can start the optimization. First we define a function for the gradient of the objective and the objective function itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are finding the gradient for the objective function\n",
    "def objective_gradient(Y, U, V):\n",
    "    gU = pd.DataFrame(np.zeros((U.shape)), index=U.index)\n",
    "    #print(gU)\n",
    "    gV = pd.DataFrame(np.zeros((V.shape)), index=V.index)\n",
    "    #print(gV)\n",
    "    obj = 0.\n",
    "    nrows = Y.shape[0]\n",
    "    for i in range(nrows):\n",
    "        row = Y.iloc[i]\n",
    "        user = row['users']\n",
    "        film = row['movies']\n",
    "        rating = row['ratings']\n",
    "        prediction = np.dot(U.loc[user], V.loc[film]) # vTu\n",
    "        diff = prediction - rating # vTu - y\n",
    "        obj += diff*diff\n",
    "        gU.loc[user] += 2*diff*V.loc[film]\n",
    "        gV.loc[film] += 2*diff*U.loc[user]\n",
    "    return obj, gU, gV\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write our simple optimisation route. This allows us to observe the objective function as the optimization proceeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 Objective function:  641.3829997326253\n",
      "Iteration 1 Objective function:  641.3826968544219\n",
      "Iteration 2 Objective function:  641.3822206592056\n",
      "Iteration 3 Objective function:  641.3813209907053\n",
      "Iteration 4 Objective function:  641.3795347339737\n",
      "Iteration 5 Objective function:  641.3759183021803\n",
      "Iteration 6 Objective function:  641.3685104429654\n",
      "Iteration 7 Objective function:  641.353208279367\n",
      "Iteration 8 Objective function:  641.3213977702343\n",
      "Iteration 9 Objective function:  641.2549476329442\n",
      "Iteration 10 Objective function:  641.1156255131904\n",
      "Iteration 11 Objective function:  640.8227330800368\n",
      "Iteration 12 Objective function:  640.2059730872627\n",
      "Iteration 13 Objective function:  638.9068297526185\n",
      "Iteration 14 Objective function:  636.1756827435473\n",
      "Iteration 15 Objective function:  630.4704067738365\n",
      "Iteration 16 Objective function:  618.7335140862577\n",
      "Iteration 17 Objective function:  595.3953391782277\n",
      "Iteration 18 Objective function:  552.2294435784714\n",
      "Iteration 19 Objective function:  483.38518936618453\n"
     ]
    }
   ],
   "source": [
    "iterations = 20\n",
    "for i in range(iterations):\n",
    "    obj, gU, gV = objective_gradient(Y, U, V)\n",
    "    print(\"Iteration\", i, \"Objective function: \", obj)\n",
    "    U -= learn_rate*gU\n",
    "    V -= learn_rate*gV \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Question 5\n",
    "\n",
    "What happens as you increase the number of iterations? What happens if you increase the learning rate?\n",
    "\n",
    "*10 marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5 Answer\n",
    "\n",
    "Write your answer to the question in this box.\n",
    "\n",
    "As we increase the number of iterations, the objective function decreases. The number of iterations should be increased until we hopefully end up getting the objective function minimum.\n",
    "\n",
    "Learning rate controls how big a step we take downhill while creating descent. If learning rate is too small, then the gradient descent can be slow. Similarly if the gradient descent is too high, then gradient descent can overshoot the global minimum. So, as the learning rate is increased, it increases the step size and thus try to converge the local minimum faster. Thus learning rate is always selected with experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 Objective function:  400.59961911092915\n",
      "Iteration 1 Objective function:  337.2501480420378\n",
      "Iteration 2 Objective function:  304.5667569434444\n",
      "Iteration 3 Objective function:  284.57559239430213\n",
      "Iteration 4 Objective function:  269.3613609212295\n",
      "Iteration 5 Objective function:  256.22654842702553\n",
      "Iteration 6 Objective function:  243.09903098576504\n",
      "Iteration 7 Objective function:  229.18809155612928\n",
      "Iteration 8 Objective function:  214.16118346364348\n",
      "Iteration 9 Objective function:  197.5855682032214\n",
      "Iteration 10 Objective function:  179.32536011016077\n",
      "Iteration 11 Objective function:  160.09972889457924\n",
      "Iteration 12 Objective function:  141.3007098948715\n",
      "Iteration 13 Objective function:  124.14649237083586\n",
      "Iteration 14 Objective function:  109.14206156005156\n",
      "Iteration 15 Objective function:  96.35875893702607\n",
      "Iteration 16 Objective function:  85.8513519230206\n",
      "Iteration 17 Objective function:  77.60890601790979\n",
      "Iteration 18 Objective function:  71.34555215659444\n",
      "Iteration 19 Objective function:  66.57456136830307\n",
      "Iteration 20 Objective function:  62.861932620196704\n",
      "Iteration 21 Objective function:  59.9409051417354\n",
      "Iteration 22 Objective function:  57.655173815931384\n",
      "Iteration 23 Objective function:  55.87857865188605\n",
      "Iteration 24 Objective function:  54.48722028373723\n",
      "Iteration 25 Objective function:  53.3689301073047\n",
      "Iteration 26 Objective function:  52.436683158229336\n",
      "Iteration 27 Objective function:  51.631450143720336\n",
      "Iteration 28 Objective function:  50.91669518220153\n",
      "Iteration 29 Objective function:  50.27071712079267\n",
      "Iteration 30 Objective function:  49.680439558388834\n",
      "Iteration 31 Objective function:  49.13742951515799\n",
      "Iteration 32 Objective function:  48.63567054075094\n",
      "Iteration 33 Objective function:  48.170423781452094\n",
      "Iteration 34 Objective function:  47.73768236502915\n",
      "Iteration 35 Objective function:  47.33392341486696\n",
      "Iteration 36 Objective function:  46.95599943524428\n",
      "Iteration 37 Objective function:  46.601089979721486\n",
      "Iteration 38 Objective function:  46.26667623910299\n",
      "Iteration 39 Objective function:  45.950522083018534\n",
      "Iteration 40 Objective function:  45.650655158172995\n",
      "Iteration 41 Objective function:  45.36534627083034\n",
      "Iteration 42 Objective function:  45.093087225882\n",
      "Iteration 43 Objective function:  44.83256795530659\n",
      "Iteration 44 Objective function:  44.58265384548077\n",
      "Iteration 45 Objective function:  44.342364014406776\n",
      "Iteration 46 Objective function:  44.11085106548323\n",
      "Iteration 47 Objective function:  43.887382630500646\n",
      "Iteration 48 Objective function:  43.67132484038456\n",
      "Iteration 49 Objective function:  43.46212773396871\n",
      "Iteration 50 Objective function:  43.25931252840852\n",
      "Iteration 51 Objective function:  43.06246062167272\n",
      "Iteration 52 Objective function:  42.87120416938262\n",
      "Iteration 53 Objective function:  42.68521806762244\n",
      "Iteration 54 Objective function:  42.504213174282846\n",
      "Iteration 55 Objective function:  42.327930609571446\n",
      "Iteration 56 Objective function:  42.15613698838409\n",
      "Iteration 57 Objective function:  41.98862045114942\n",
      "Iteration 58 Objective function:  41.82518737416254\n",
      "Iteration 59 Objective function:  41.66565965447105\n",
      "Iteration 60 Objective function:  41.5098724776007\n",
      "Iteration 61 Objective function:  41.35767248856128\n",
      "Iteration 62 Objective function:  41.20891629757737\n",
      "Iteration 63 Objective function:  41.063469261836985\n",
      "Iteration 64 Objective function:  40.92120449328979\n",
      "Iteration 65 Objective function:  40.78200205023693\n",
      "Iteration 66 Objective function:  40.645748277208845\n",
      "Iteration 67 Objective function:  40.51233526351566\n",
      "Iteration 68 Objective function:  40.38166039596\n",
      "Iteration 69 Objective function:  40.25362598559729\n",
      "Iteration 70 Objective function:  40.12813895219134\n",
      "Iteration 71 Objective function:  40.00511055321009\n",
      "Iteration 72 Objective function:  39.884456146898415\n",
      "Iteration 73 Objective function:  39.766094981221094\n",
      "Iteration 74 Objective function:  39.64995000232533\n",
      "Iteration 75 Objective function:  39.53594767770446\n",
      "Iteration 76 Objective function:  39.4240178304748\n",
      "Iteration 77 Objective function:  39.314093482165276\n",
      "Iteration 78 Objective function:  39.206110702194266\n",
      "Iteration 79 Objective function:  39.10000846281002\n",
      "Iteration 80 Objective function:  38.995728498721064\n",
      "Iteration 81 Objective function:  38.89321517098051\n",
      "Iteration 82 Objective function:  38.79241533492736\n",
      "Iteration 83 Objective function:  38.69327821214657\n",
      "Iteration 84 Objective function:  38.595755266517486\n",
      "Iteration 85 Objective function:  38.499800084474266\n",
      "Iteration 86 Objective function:  38.40536825962576\n",
      "Iteration 87 Objective function:  38.31241728188359\n",
      "Iteration 88 Objective function:  38.220906431226226\n",
      "Iteration 89 Objective function:  38.13079667620015\n",
      "Iteration 90 Objective function:  38.04205057722401\n",
      "Iteration 91 Objective function:  37.95463219472279\n",
      "Iteration 92 Objective function:  37.86850700208167\n",
      "Iteration 93 Objective function:  37.783641803374046\n",
      "Iteration 94 Objective function:  37.70000465578211\n",
      "Iteration 95 Objective function:  37.6175647966028\n",
      "Iteration 96 Objective function:  37.536292574702586\n",
      "Iteration 97 Objective function:  37.45615938626637\n",
      "Iteration 98 Objective function:  37.3771376146673\n",
      "Iteration 99 Objective function:  37.29920057427235\n"
     ]
    }
   ],
   "source": [
    "# Question 5 Code Answer\n",
    "iterations = 100\n",
    "for i in range(iterations):\n",
    "    obj, gU, gV = objective_gradient(Y, U, V)\n",
    "    print(\"Iteration\", i, \"Objective function: \", obj)\n",
    "    U -= learn_rate*gU\n",
    "    V -= learn_rate*gV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "Predictions can be made from the model of the appropriate rating for a given user, $i$, for a given film, $j$, by simply taking the inner product between their vectors $\\mathbf{u}_i$ and $\\mathbf{v}_j$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Question 6\n",
    "\n",
    "Create a function that provides the prediction of the ratings for the users in the dataset. Is the quality of the predictions affected by the number of iterations or the learning rate? The function should receive `Y`, `U` and `V` and return the predictions and the absolute error between the predictions and the actual rating given by the users. The predictions and the absolute error should be added as additional columns to the dataframe `Y`.\n",
    "\n",
    "*15 marks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     movies   ratings  ratingsorig  users  Prediction     Error\n",
      "0         6  0.238318          4.0   23.0    0.238120  0.000198\n",
      "1        29  0.238318          4.0   23.0    0.238108  0.000210\n",
      "2        32 -0.261682          3.5   23.0   -0.302808  0.041126\n",
      "3        50  0.238318          4.0   23.0   -0.306583  0.544900\n",
      "4        58 -0.761682          3.0   23.0   -0.761030  0.000652\n",
      "5       175 -0.761682          3.0   23.0   -0.761013  0.000669\n",
      "6       272  0.238318          4.0   23.0    0.238117  0.000201\n",
      "7       293  0.238318          4.0   23.0    0.270961  0.032644\n",
      "8       296 -0.261682          3.5   23.0    0.069323  0.331006\n",
      "9       300 -0.761682          3.0   23.0   -0.619041  0.142642\n",
      "10      334  0.238318          4.0   23.0    0.238104  0.000214\n",
      "11      431 -0.761682          3.0   23.0   -0.761021  0.000662\n",
      "12      454 -1.261682          2.5   23.0   -0.658313  0.603369\n",
      "13      541  1.238318          5.0   23.0    0.753048  0.485270\n",
      "14      608 -0.261682          3.5   23.0   -0.302768  0.041086\n",
      "15      741  0.238318          4.0   23.0    0.238112  0.000206\n",
      "16      750 -0.761682          3.0   23.0   -0.761022  0.000661\n",
      "17      778  0.238318          4.0   23.0    0.096235  0.142083\n",
      "18      858 -0.261682          3.5   23.0    0.177771  0.439453\n",
      "19      866 -0.761682          3.0   23.0   -0.761029  0.000653\n",
      "20      904 -0.261682          3.5   23.0   -0.261457  0.000225\n",
      "21      912 -0.261682          3.5   23.0    0.151962  0.413645\n",
      "22      913  0.238318          4.0   23.0    0.238113  0.000205\n",
      "23      923  0.238318          4.0   23.0    0.591331  0.353013\n",
      "24      924  0.238318          4.0   23.0   -0.143134  0.381451\n",
      "25     1036 -0.761682          3.0   23.0   -0.088529  0.673153\n",
      "26     1050 -0.261682          3.5   23.0   -0.261459  0.000223\n",
      "27     1080 -0.261682          3.5   23.0   -0.121568  0.140115\n",
      "28     1089  0.238318          4.0   23.0    0.238103  0.000215\n",
      "29     1136 -0.761682          3.0   23.0   -0.759111  0.002571\n",
      "..      ...       ...          ...    ...         ...       ...\n",
      "826    4226  0.238318          4.0  570.0   -0.107506  0.345824\n",
      "827    4306  0.238318          4.0  570.0   -0.201360  0.439677\n",
      "828    4878  0.738318          4.5  570.0    0.682222  0.056096\n",
      "829    4886 -0.261682          3.5  570.0   -0.232244  0.029439\n",
      "830    4993  0.238318          4.0  570.0    0.203789  0.034529\n",
      "831    5349  0.238318          4.0  570.0    0.233748  0.004570\n",
      "832    5445  0.738318          4.5  570.0    0.674942  0.063376\n",
      "833    5902  0.238318          4.0  570.0    0.233746  0.004572\n",
      "834    5989 -0.261682          3.5  570.0   -0.218444  0.043238\n",
      "835    6323  0.238318          4.0  570.0    0.172181  0.066137\n",
      "836    6333  0.238318          4.0  570.0    0.233733  0.004585\n",
      "837    6539 -0.261682          3.5  570.0   -0.190595  0.071088\n",
      "838    6942  0.738318          4.5  570.0    0.724137  0.014180\n",
      "839    6953  0.238318          4.0  570.0    0.233749  0.004569\n",
      "840    7153 -0.261682          3.5  570.0    0.079792  0.341475\n",
      "841    7254  0.238318          4.0  570.0    0.233733  0.004585\n",
      "842    7361  0.238318          4.0  570.0    0.233739  0.004579\n",
      "843    8636 -0.261682          3.5  570.0    0.102921  0.364603\n",
      "844   33794 -0.261682          3.5  570.0   -0.256663  0.005019\n",
      "845   45722  0.238318          4.0  570.0    0.233736  0.004581\n",
      "846   47099  0.238318          4.0  570.0    0.233739  0.004579\n",
      "847   48516 -0.261682          3.5  570.0   -0.256658  0.005024\n",
      "848   48738 -0.261682          3.5  570.0   -0.256674  0.005009\n",
      "849   48774  0.238318          4.0  570.0    0.233756  0.004561\n",
      "850   48780 -0.261682          3.5  570.0   -0.256663  0.005019\n",
      "851   49272  0.238318          4.0  570.0    0.233739  0.004579\n",
      "852   49530 -0.261682          3.5  570.0   -0.256651  0.005031\n",
      "853   52328 -0.261682          3.5  570.0   -0.256653  0.005029\n",
      "854   53000 -0.761682          3.0  570.0   -0.747062  0.014621\n",
      "855   53125 -0.761682          3.0  570.0   -0.747044  0.014639\n",
      "\n",
      "[856 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Question 6 Code Answer\n",
    "\n",
    "def prediction(Y, U, V):\n",
    "    new_prediction = np.zeros((Y.shape[0]))\n",
    "    new_abs_error = np.zeros((Y.shape[0]))\n",
    "    sum_error = 0\n",
    "    for i in range(Y.shape[0]):\n",
    "        row = Y.iloc[i]\n",
    "        user = row['users']\n",
    "        film = row['movies']\n",
    "        rating = row['ratings']\n",
    "        prediction = np.dot(U.loc[user], V.loc[film]) # vTu\n",
    "        error = prediction - rating # vTu - y\n",
    "        abs_error = abs(error)  # taking absolute value of the error calculated\n",
    "        new_prediction[i] = prediction\n",
    "        new_abs_error[i] = abs_error\n",
    "    Y['Prediction'] = new_prediction\n",
    "    Y['Error'] = new_abs_error\n",
    "       \n",
    "    return(new_prediction, new_abs_error)\n",
    "\n",
    "new_prediction,new_abs_error = prediction(Y, U, V)\n",
    "print(Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer to que 6:\n",
    "\n",
    "As asked in the question, the prediction values do get affected by learning rate and iteration values.\n",
    "As I increased the iteration value from 50 to 100, the decrease in the error value was observed. \n",
    "Similarly when learning rate was increased from 0.02 to 0.03, then the error was increased by small values.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent or Robbins Monroe Algorithm\n",
    "\n",
    "Stochastic gradient descent involves updating separating each gradient update according to each separate observation, rather than summing over them all. It is an approximate optimization method, but it has proven convergence under certain conditions and can be much faster in practice. It is used widely by internet companies for doing machine learning in practice. For example, Facebook's ad ranking algorithm uses stochastic gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Question 7\n",
    "\n",
    "Create a stochastic gradient descent version of the algorithm. Monitor the objective function after every 1000 updates to ensure that it is decreasing. When you have finished, plot the movie map and the user map in two dimensions (you can use the columns of the matrices $\\mathbf{U}$ for the user map and the columns of $\\mathbf{V}$ for the movie map). Provide three observations about these maps.\n",
    "\n",
    "*25 marks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For each update 1 Objective function:  78.31930644224558\n",
      "For each update 2 Objective function:  187.5716297298652\n",
      "For each update 3 Objective function:  214.4443414243847\n",
      "For each update 4 Objective function:  30.912133724224987\n",
      "For each update 5 Objective function:  2.505175572119502\n",
      "For each update 7 Objective function:  4.090456367352065\n",
      "For each update 8 Objective function:  1.7159011344407138\n",
      "For each update 9 Objective function:  0.980735731156392\n",
      "For each update 10 Objective function:  67.80117451738184\n",
      "For each update 11 Objective function:  0.2958454761385223\n",
      "For each update 12 Objective function:  30.580376283388592\n",
      "For each update 14 Objective function:  0.7159295849355682\n",
      "For each update 15 Objective function:  34.01445409172702\n",
      "For each update 16 Objective function:  0.5735317966368453\n",
      "For each update 17 Objective function:  0.5326301241129553\n",
      "For each update 18 Objective function:  219.71020696330496\n",
      "For each update 19 Objective function:  6.913288871123336\n",
      "For each update 21 Objective function:  5.113868952289479\n",
      "For each update 22 Objective function:  4.8154378287965\n",
      "For each update 23 Objective function:  0.5251474151632303\n",
      "For each update 24 Objective function:  5.520318884888696\n",
      "For each update 25 Objective function:  11.237576657695527\n",
      "For each update 26 Objective function:  36.31354057419675\n",
      "For each update 28 Objective function:  10.592613062846508\n",
      "For each update 29 Objective function:  0.3925848969723045\n",
      "For each update 30 Objective function:  0.437756359524404\n",
      "For each update 31 Objective function:  0.39100382943186957\n",
      "For each update 32 Objective function:  11.946604930946219\n",
      "For each update 33 Objective function:  210.06665137776912\n",
      "For each update 35 Objective function:  2.504430069360539\n",
      "For each update 36 Objective function:  0.3231169662312214\n",
      "For each update 37 Objective function:  13.308417080393482\n",
      "For each update 38 Objective function:  0.24308324224838504\n",
      "For each update 39 Objective function:  0.3066032943850981\n",
      "            0         1\n",
      "23  -0.923872  0.901016\n",
      "75  -0.165753  1.712166\n",
      "296  1.618198 -0.717170\n",
      "547 -0.000741  0.000853\n",
      "31  -0.371831  0.598349\n",
      "562  0.486520  1.671406\n",
      "179  0.396230  1.403653\n",
      "46  -1.408599 -0.025429\n",
      "193 -1.024974  0.735736\n",
      "570 -1.431526  0.564317\n",
      "               0         1\n",
      "1      -0.002492  0.000806\n",
      "2      -0.000736  0.000991\n",
      "3      -0.000308 -0.000079\n",
      "5       0.002190  0.000107\n",
      "6       0.000211  0.000880\n",
      "7      -0.256017  0.412252\n",
      "9      -0.001071  0.000115\n",
      "10     -0.000455 -0.000959\n",
      "11      0.001652 -0.002025\n",
      "17      0.002124  0.000579\n",
      "18      0.000634 -0.000469\n",
      "19      0.000949  0.000636\n",
      "25     -0.000068 -0.000844\n",
      "29      0.000279 -0.000115\n",
      "32      0.000805  0.000454\n",
      "34      0.001317  0.000083\n",
      "39      0.001985  0.001450\n",
      "47     -0.000935 -0.000107\n",
      "50     -0.000360 -0.000698\n",
      "52     -0.002412 -0.002353\n",
      "58      0.001162  0.001068\n",
      "62     -0.001300 -0.001344\n",
      "65      0.000235 -0.000595\n",
      "95     -0.000495 -0.000946\n",
      "104    -0.001229 -0.002099\n",
      "110     0.231601  0.816833\n",
      "111    -0.000054  0.000249\n",
      "112    -0.000306  0.000478\n",
      "113     0.000499  0.000106\n",
      "141    -0.001535  0.000651\n",
      "...          ...       ...\n",
      "68157  -0.000742  0.001257\n",
      "70336  -0.000481  0.000325\n",
      "71899   0.000341  0.000782\n",
      "73211   0.000020 -0.000326\n",
      "74324   0.000602 -0.000516\n",
      "78499   0.000328 -0.000480\n",
      "79132   0.001289 -0.001008\n",
      "79695   0.000316 -0.000376\n",
      "80831   0.000272 -0.000210\n",
      "80860   0.000886 -0.001622\n",
      "82041  -0.001185  0.001204\n",
      "82167  -0.002477 -0.001591\n",
      "83134   0.002597 -0.000238\n",
      "84374   0.000540 -0.000933\n",
      "86347   0.001713 -0.000744\n",
      "87306   0.000131  0.001097\n",
      "88129   0.000207 -0.000113\n",
      "88163   0.001231 -0.000834\n",
      "91485  -0.000506 -0.000004\n",
      "93840  -0.000881  0.000588\n",
      "97921  -0.000550  0.000236\n",
      "97938   0.000889 -0.001025\n",
      "99007   0.000848  0.000557\n",
      "99191  -0.000949 -0.000241\n",
      "100306 -0.000296 -0.000380\n",
      "160848 -0.001125  0.000733\n",
      "166528 -0.000316 -0.000601\n",
      "169034  0.001463  0.000945\n",
      "171765  0.000475 -0.000435\n",
      "180031 -0.134867  0.060573\n",
      "\n",
      "[590 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Question 7 Code Answer\n",
    "\n",
    "iteration = 40\n",
    "q = 2 # the dimension of our map of the 'library'\n",
    "learn_rate = 0.02\n",
    "\n",
    "Y = Y.sample(frac=1).reset_index(drop=True)  # shuffle the dataset\n",
    "U = pd.DataFrame(np.random.normal(size=(nUsersInExample, q))*0.001, index=my_batch_users)\n",
    "V = pd.DataFrame(np.random.normal(size=(n_movies, q))*0.001, index=indexes_unique_movies)\n",
    "counter = 0 # variable to keep a count of 1000 updtaes\n",
    "nrows = Y.shape[0]\n",
    "\n",
    "for i in range(iteration):   # to improve values of U and V, iteration has been done over entire dataset \n",
    "    obj = 0.\n",
    "    for j in range (nrows):  # for stochastic gradient, update the parameter U and V for each dataset value\n",
    "        gU = pd.DataFrame(np.zeros((U.shape)), index=U.index)\n",
    "        #print(gU)\n",
    "        gV = pd.DataFrame(np.zeros((V.shape)), index=V.index)\n",
    "        #print(gV)\n",
    "        \n",
    "        row = Y.iloc[i]\n",
    "        user = row['users']\n",
    "        film = row['movies']\n",
    "        rating = row['ratings']\n",
    "        prediction = np.dot(U.loc[user], V.loc[film]) # vTu\n",
    "        diff = prediction - rating # vTu - y\n",
    "        obj += diff*diff\n",
    "        gU.loc[user] += 2*diff*V.loc[film]\n",
    "        gV.loc[film] += 2*diff*U.loc[user]\n",
    "        U -= learn_rate*gU\n",
    "        V -= learn_rate*gV \n",
    "        counter = counter + 1\n",
    "        if(counter == 1000):\n",
    "            print(\"For each update\", i, \"Objective function: \", obj)\n",
    "            counter = 0\n",
    "print(U)  # final value of U after completing over 10 iterations\n",
    "print(V)  # final value of V after completing over 10 iterations\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10ed976a0>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ba9eb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "   \n",
    "%matplotlib inline\n",
    "import pylab as plt\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "plt.plot(U.iloc[:,0], U.iloc[:,1], 'rx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10f22f400>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ed06ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "plt.plot(V.iloc[:,0], V.iloc[:,1], 'bx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 7 \n",
    "\n",
    "The observations are:\n",
    "    1. In movie map, the dense area represents the movies that are rated by same type of the users.\n",
    "    2. The points which are away from the dense area in movie map represents that these are the movies that are not rated by same type of users.\n",
    "    3. In user map, the dense area(the points that are below 0 on X-axis) represents that, the user have same movie interest. Whereas the scattered point at corner shows that the user have different movie interest.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is Our Map Enough? Are Our Data Enough?\n",
    "\n",
    "Is two dimensions really enough to capture the complexity of humans and their artforms? Perhaps we need even more dimensions to capture that complexity. Extending our books analogy further, consider how we should place books that have a historical timeframe as well as some geographical location. Do we really want books from the 2nd World War to sit alongside books from the Roman Empire? Books on the American invasion of Sicily in 1943 are perhaps less related to books about Carthage than those that study the Jewish Revolt from 66-70 (in the Roman Province of Judaea). So books that relate to subjects which are closer in time should be stored together. However, a student of rebellion against empire may also be interested in the relationship between the Jewish Revolt of 66-70 and the Indian Rebellion of 1857, nearly 1800 years later. Whilst the technologies are different, the psychology of the people is shared: a rebellious nation angainst their imperial masters, triggered by misrule with a religious and cultural background. To capture such complexities we would need further dimensions in our latent representation. But are further dimensions justified by the amount of data we have? Can we really understand the facets of a film that only has at most three or four ratings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going Further\n",
    "\n",
    "If you want to take this model further then you'll need more data. You can use again the MovieLens 100k data but increasing the number of users (for example, for the Steepest Descent Algorithm you can do this by modifying the variable `nUsersInExample` that was set as 10 before)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Question 8\n",
    "\n",
    "Use stochastic gradient descent to make a movie map for the MovieLens 100k data. Plot the map of the movies when you are finished.\n",
    "\n",
    "*15 marks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0    1    2    3    4    5    6    7    8    9   ...   600  601  602  \\\n",
      "0     NaN  NaN  NaN  NaN  5.0  4.5  4.0  5.0  2.0  4.0 ...   4.0  5.0  5.0   \n",
      "1     NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  3.5 ...   2.0  NaN  NaN   \n",
      "2     NaN  NaN  NaN  NaN  NaN  NaN  4.0  NaN  NaN  NaN ...   1.0  3.0  NaN   \n",
      "3     NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "4     NaN  NaN  NaN  NaN  3.0  NaN  NaN  NaN  NaN  NaN ...   NaN  3.0  NaN   \n",
      "5     4.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   2.0  NaN  3.0   \n",
      "6     NaN  NaN  NaN  NaN  4.0  NaN  3.0  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "7     NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "8     NaN  NaN  NaN  NaN  NaN  NaN  3.0  NaN  NaN  NaN ...   3.0  4.0  NaN   \n",
      "9     NaN  NaN  NaN  NaN  4.0  NaN  NaN  3.0  NaN  3.5 ...   4.0  NaN  3.0   \n",
      "10    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  3.5 ...   NaN  NaN  3.0   \n",
      "11    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   3.0  NaN  NaN   \n",
      "12    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "13    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "14    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "15    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "16    NaN  NaN  NaN  NaN  3.0  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "17    NaN  NaN  NaN  NaN  NaN  NaN  4.0  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "18    NaN  NaN  NaN  NaN  NaN  NaN  3.0  NaN  NaN  NaN ...   1.0  NaN  NaN   \n",
      "19    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   2.0  NaN  NaN   \n",
      "20    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  3.0   \n",
      "21    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   3.0  NaN  NaN   \n",
      "22    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "23    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "24    NaN  NaN  NaN  NaN  2.0  NaN  NaN  NaN  NaN  4.0 ...   NaN  NaN  NaN   \n",
      "25    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "26    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "27    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "28    4.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "29    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ... ...   ...  ...  ...   \n",
      "9532  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "9533  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "9534  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "9535  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "9536  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "9537  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "9538  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "9539  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "9540  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "9541  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "9542  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "9543  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "9544  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "9545  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "9546  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "9547  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "9548  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "9549  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "9550  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "9551  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "9552  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "9553  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "9554  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "9555  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "9556  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "9557  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "9558  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "9559  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "9560  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "9561  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
      "\n",
      "      603  604  605  606  607  608  609  \n",
      "0     NaN  4.0  3.0  3.5  NaN  4.0  NaN  \n",
      "1     NaN  NaN  NaN  NaN  NaN  2.5  NaN  \n",
      "2     NaN  NaN  NaN  NaN  NaN  3.5  NaN  \n",
      "3     NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "4     NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "5     NaN  NaN  4.0  NaN  NaN  NaN  NaN  \n",
      "6     NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "7     NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "8     NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9     NaN  NaN  3.0  NaN  NaN  2.5  NaN  \n",
      "10    NaN  4.0  NaN  NaN  NaN  NaN  NaN  \n",
      "11    NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "12    NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "13    NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "14    NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "15    NaN  NaN  NaN  NaN  NaN  4.5  NaN  \n",
      "16    NaN  5.0  NaN  NaN  NaN  NaN  NaN  \n",
      "17    NaN  NaN  NaN  NaN  NaN  3.0  NaN  \n",
      "18    NaN  NaN  NaN  NaN  NaN  4.0  NaN  \n",
      "19    NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "20    NaN  NaN  NaN  NaN  NaN  2.5  NaN  \n",
      "21    NaN  NaN  NaN  NaN  NaN  2.0  NaN  \n",
      "22    NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "23    NaN  3.0  NaN  NaN  NaN  2.0  NaN  \n",
      "24    NaN  NaN  5.0  NaN  NaN  NaN  NaN  \n",
      "25    NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "26    NaN  NaN  NaN  NaN  NaN  2.5  NaN  \n",
      "27    NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "28    NaN  NaN  5.0  NaN  NaN  NaN  NaN  \n",
      "29    NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "...   ...  ...  ...  ...  ...  ...  ...  \n",
      "9532  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9533  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9534  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9535  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9536  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9537  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9538  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9539  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9540  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9541  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9542  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9543  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9544  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9545  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9546  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9547  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9548  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9549  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9550  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9551  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9552  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9553  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9554  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9555  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9556  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9557  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9558  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9559  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9560  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9561  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "\n",
      "[9562 rows x 610 columns]\n"
     ]
    }
   ],
   "source": [
    "# Code for question 8 here\n",
    "# the code is divided into different code cells\n",
    "# 1. To find the Y matrix for all the users.\n",
    "\n",
    "YourStudentID = 356  # Include here the last three digits of your UCard number\n",
    "\n",
    "ratings = pd.read_csv(\"./ml-latest-small/ratings.csv\") \n",
    "\"\"\"\n",
    "ratings is a DataFrame with four columns: userId, movieId, rating and tags. We\n",
    "first want to identify how many unique users there are. We can use the unique \n",
    "method in pandas\n",
    "\"\"\"\n",
    "indexes_unique_users = ratings['userId'].unique()\n",
    "n_users = indexes_unique_users.shape[0]   #shape is used to get the current shape of the array\n",
    "nUsersInExample = n_users\n",
    "\"\"\" \n",
    "We randomly select 'nUsers' users with their ratings. We first fix the seed\n",
    "of the random generator to make sure that we always get the same 'nUsers'\n",
    "\"\"\"\n",
    "np.random.seed(YourStudentID)\n",
    "indexes_users = np.random.permutation(n_users)  #Randomly permute a sequence, or return a permuted range.\n",
    "my_batch_users = indexes_users[0:nUsersInExample]\n",
    "#print(my_batch_users)\n",
    "\"\"\"\n",
    "We will use now the list of 'my_batch_users' to create a matrix Y. \n",
    "\"\"\"\n",
    "# We need to make a list of the movies that these users have watched\n",
    "list_movies_each_user = [[] for _ in range(nUsersInExample)]\n",
    "list_ratings_each_user = [[] for _ in range(nUsersInExample)]\n",
    "# Movies\n",
    "list_movies = ratings['movieId'][ratings['userId'] == my_batch_users[0]].values\n",
    "list_movies_each_user[0] = list_movies \n",
    "#print(list_movies)\n",
    "# Ratings                      \n",
    "list_ratings = ratings['rating'][ratings['userId'] == my_batch_users[0]].values\n",
    "list_ratings_each_user[0] = list_ratings\n",
    "#print(list_ratings)\n",
    "# Users\n",
    "n_each_user = list_movies.shape[0]\n",
    "#print(n_each_user)\n",
    "list_users = my_batch_users[0]*np.ones((1, n_each_user)) #np.ones > Return a new array of given shape and type, filled with ones\n",
    "\n",
    "\n",
    "for i in range(1, nUsersInExample):\n",
    "    # Movies\n",
    "    local_list_per_user_movies = ratings['movieId'][ratings['userId'] == my_batch_users[i]].values\n",
    "    list_movies_each_user[i] = local_list_per_user_movies\n",
    "    list_movies = np.append(list_movies,local_list_per_user_movies)\n",
    "    # Ratings                                 \n",
    "    local_list_per_user_ratings = ratings['rating'][ratings['userId'] == my_batch_users[i]].values\n",
    "    list_ratings_each_user[i] = local_list_per_user_ratings\n",
    "    list_ratings = np.append(list_ratings, local_list_per_user_ratings)  \n",
    "    # Users                                   \n",
    "    n_each_user = local_list_per_user_movies.shape[0]                                                                               \n",
    "    local_rep_user =  my_batch_users[i]*np.ones((1, n_each_user))    \n",
    "    list_users = np.append(list_users, local_rep_user)\n",
    "\n",
    "# Let us first see how many unique movies have been rated\n",
    "indexes_unique_movies = np.unique(list_movies)\n",
    "n_movies = indexes_unique_movies.shape[0]\n",
    "#print(n_movies)\n",
    "# As it is expected no all users have rated all movies. We will build a matrix Y \n",
    "# with NaN inputs and fill according to the data for each user \n",
    "temp = np.empty((n_movies,nUsersInExample,))\n",
    "temp[:] = np.nan\n",
    "Y_with_NaNs = pd.DataFrame(temp)\n",
    "for i in range(nUsersInExample):\n",
    " local_movies = list_movies_each_user[i]\n",
    " ixs = np.in1d(indexes_unique_movies, local_movies)\n",
    " Y_with_NaNs.loc[ixs, i] = list_ratings_each_user[i]\n",
    "print(Y_with_NaNs)\n",
    "Y_with_NaNs.index = indexes_unique_movies.tolist()\n",
    "Y_with_NaNs.columns = my_batch_users.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       movies   ratings  ratingsorig  users\n",
      "0           6  0.500889          4.0   23.0\n",
      "1          29  0.500889          4.0   23.0\n",
      "2          32  0.000889          3.5   23.0\n",
      "3          50  0.500889          4.0   23.0\n",
      "4          58 -0.499111          3.0   23.0\n",
      "5         175 -0.499111          3.0   23.0\n",
      "6         272  0.500889          4.0   23.0\n",
      "7         293  0.500889          4.0   23.0\n",
      "8         296  0.000889          3.5   23.0\n",
      "9         300 -0.499111          3.0   23.0\n",
      "10        334  0.500889          4.0   23.0\n",
      "11        431 -0.499111          3.0   23.0\n",
      "12        454 -0.999111          2.5   23.0\n",
      "13        541  1.500889          5.0   23.0\n",
      "14        608  0.000889          3.5   23.0\n",
      "15        741  0.500889          4.0   23.0\n",
      "16        750 -0.499111          3.0   23.0\n",
      "17        778  0.500889          4.0   23.0\n",
      "18        858  0.000889          3.5   23.0\n",
      "19        866 -0.499111          3.0   23.0\n",
      "20        904  0.000889          3.5   23.0\n",
      "21        912  0.000889          3.5   23.0\n",
      "22        913  0.500889          4.0   23.0\n",
      "23        923  0.500889          4.0   23.0\n",
      "24        924  0.500889          4.0   23.0\n",
      "25       1036 -0.499111          3.0   23.0\n",
      "26       1050  0.000889          3.5   23.0\n",
      "27       1080  0.000889          3.5   23.0\n",
      "28       1089  0.500889          4.0   23.0\n",
      "29       1136 -0.499111          3.0   23.0\n",
      "...       ...       ...          ...    ...\n",
      "99504   65088  0.500889          4.0  307.0\n",
      "99505   65601 -1.499111          2.0  307.0\n",
      "99506   65738  0.500889          4.0  307.0\n",
      "99507   65740  0.500889          4.0  307.0\n",
      "99508   65882  0.000889          3.5  307.0\n",
      "99509   66097  1.000889          4.5  307.0\n",
      "99510   67408  0.500889          4.0  307.0\n",
      "99511   68952  1.000889          4.5  307.0\n",
      "99512   68954  1.500889          5.0  307.0\n",
      "99513   69122  1.500889          5.0  307.0\n",
      "99514    1013  0.000889          3.5  320.0\n",
      "99515    2058  0.500889          4.0  320.0\n",
      "99516    3404 -2.999111          0.5  320.0\n",
      "99517    3534  0.500889          4.0  320.0\n",
      "99518    3578  0.000889          3.5  320.0\n",
      "99519    3703 -0.499111          3.0  320.0\n",
      "99520    3745  0.000889          3.5  320.0\n",
      "99521    4387  0.500889          4.0  320.0\n",
      "99522    5785  0.500889          4.0  320.0\n",
      "99523    6283  0.000889          3.5  320.0\n",
      "99524    6857  0.000889          3.5  320.0\n",
      "99525   26555  0.000889          3.5  320.0\n",
      "99526   31184  0.500889          4.0  320.0\n",
      "99527   42718  0.000889          3.5  320.0\n",
      "99528   59315  0.500889          4.0  320.0\n",
      "99529   59900 -0.499111          3.0  320.0\n",
      "99530   62999  0.000889          3.5  320.0\n",
      "99531   68358  0.500889          4.0  320.0\n",
      "99532   71535  0.500889          4.0  320.0\n",
      "99533   72998  0.500889          4.0  320.0\n",
      "\n",
      "[99534 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# 2. To remove NaN values from the above Y matrix and extract only the significant information\n",
    "\n",
    "p_list_ratings = np.concatenate(list_ratings_each_user).ravel()  # np.ravel > A 1-D array, containing the elements of the input, is returned\n",
    "p_list_ratings_original = p_list_ratings.tolist()\n",
    "mean_ratings_train = np.mean(p_list_ratings)  # returns average of array elements\n",
    "p_list_ratings =  p_list_ratings - mean_ratings_train # remove the mean\n",
    "p_list_movies = np.concatenate(list_movies_each_user).ravel().tolist()\n",
    "p_list_users = list_users.tolist()\n",
    "Y = pd.DataFrame({'users': p_list_users, 'movies': p_list_movies, 'ratingsorig': p_list_ratings_original,'ratings':p_list_ratings.tolist()})\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After iteration 0 Objective function:  145.85892131575048\n",
      "After iteration 1 Objective function:  73.64806806915178\n",
      "After iteration 2 Objective function:  150.1165801559513\n",
      "After iteration 3 Objective function:  66.23531825110405\n",
      "After iteration 4 Objective function:  181.67576700474106\n",
      "After iteration 5 Objective function:  108.93751367227189\n",
      "After iteration 6 Objective function:  169.87247220028416\n",
      "After iteration 7 Objective function:  80.5145410357124\n",
      "After iteration 8 Objective function:  189.87054814247486\n",
      "After iteration 9 Objective function:  84.98418165764716\n",
      "After iteration 10 Objective function:  85.36931571505342\n",
      "After iteration 11 Objective function:  277.5520188272186\n",
      "After iteration 12 Objective function:  101.98064915358123\n",
      "After iteration 13 Objective function:  0.06114547894360007\n",
      "After iteration 14 Objective function:  82.79876133619311\n",
      "After iteration 15 Objective function:  16.925041158105028\n",
      "After iteration 16 Objective function:  6.152426936344719e-05\n",
      "After iteration 17 Objective function:  260.4474974315811\n",
      "After iteration 18 Objective function:  278.02464259929684\n",
      "After iteration 19 Objective function:  2.078413454857521\n",
      "            0         1\n",
      "23   0.000422 -0.001138\n",
      "75   0.001304 -0.001124\n",
      "296 -0.000076  0.001468\n",
      "547  0.001118  0.000512\n",
      "31   0.000117  0.001122\n",
      "562 -0.000155  0.000080\n",
      "179  0.001703 -0.000534\n",
      "46   0.000217 -0.000419\n",
      "193 -0.000535  0.000244\n",
      "570 -0.000103 -0.000055\n",
      "493  0.000704 -0.000481\n",
      "224  0.002772 -0.000090\n",
      "428 -0.000656 -0.001166\n",
      "305 -0.000645  0.000743\n",
      "427 -0.000278 -0.001743\n",
      "468  0.001483 -0.000110\n",
      "349 -0.001380  0.000160\n",
      "170  0.000334 -0.001434\n",
      "221  0.000084  0.000888\n",
      "50   0.001465  0.000192\n",
      "89   0.000691 -0.001297\n",
      "371 -0.000458 -0.000888\n",
      "353  0.001020 -0.000526\n",
      "535 -0.000295 -0.000679\n",
      "530  0.001500  0.001520\n",
      "518 -0.000494  0.000636\n",
      "213 -0.001561  0.000417\n",
      "476 -0.000240 -0.001369\n",
      "70   0.000539  0.001336\n",
      "401  0.001014  0.001677\n",
      "..        ...       ...\n",
      "225  0.001388  0.001701\n",
      "290  0.000082  0.000684\n",
      "215 -0.000475  0.000600\n",
      "110  0.001889 -0.000998\n",
      "369  0.000355 -0.000219\n",
      "160 -0.000334 -0.000798\n",
      "572 -0.002041 -0.000830\n",
      "316 -0.000052  0.000507\n",
      "497 -0.001484  0.000039\n",
      "261  0.001462  0.000681\n",
      "131 -0.000514  0.000477\n",
      "609 -0.000750  0.001677\n",
      "600  0.000776  0.000887\n",
      "114  0.000568  0.000154\n",
      "13   0.000836 -0.000030\n",
      "286  0.000542  0.000020\n",
      "400 -0.000437  0.000646\n",
      "7   -0.001451  0.001120\n",
      "467  0.000689  0.000833\n",
      "585 -0.001319 -0.001035\n",
      "217 -0.001082 -0.000272\n",
      "456  0.001604 -0.000948\n",
      "57  -0.001788  0.000631\n",
      "52   0.000881  0.000347\n",
      "263  0.001472 -0.000099\n",
      "391  0.000506  0.000041\n",
      "567  0.001354 -0.002445\n",
      "574  0.000353  0.000477\n",
      "307 -1.224350  0.008788\n",
      "320 -0.001464  0.000477\n",
      "\n",
      "[610 rows x 2 columns]\n",
      "                   0         1\n",
      "1      -3.956830e-04  0.000294\n",
      "2      -1.648448e-03 -0.001514\n",
      "3      -1.453823e-03  0.000330\n",
      "4       5.602286e-05 -0.000484\n",
      "5      -3.356949e-04 -0.000199\n",
      "6      -6.948990e-04 -0.000494\n",
      "7      -1.155659e-03  0.000223\n",
      "8       1.990543e-04  0.000321\n",
      "9      -6.180743e-04  0.000366\n",
      "10      1.237742e-03  0.001656\n",
      "11      5.649177e-04  0.000704\n",
      "12     -4.120004e-04  0.000686\n",
      "13     -7.762370e-04 -0.000595\n",
      "14     -8.286857e-04  0.001286\n",
      "15      5.649641e-04 -0.001521\n",
      "16     -8.740505e-04 -0.000345\n",
      "17      2.081329e-04  0.001573\n",
      "18      1.282496e-03  0.001187\n",
      "19     -5.787837e-04 -0.000250\n",
      "20      1.767290e-03 -0.001422\n",
      "21     -8.756592e-04  0.000731\n",
      "22      1.213445e-03  0.000533\n",
      "23      7.799987e-04 -0.000767\n",
      "24      2.035414e-03  0.000162\n",
      "25     -4.616901e-04 -0.000718\n",
      "26     -1.316640e-03 -0.000202\n",
      "27     -1.024833e-04  0.001135\n",
      "28     -8.030282e-05 -0.000540\n",
      "29     -8.224892e-07 -0.000599\n",
      "30      6.650767e-04  0.000892\n",
      "...              ...       ...\n",
      "188189  1.033155e-03 -0.000043\n",
      "188301  1.747394e-03  0.000142\n",
      "188675  1.391524e-03  0.000262\n",
      "188751  4.989096e-04 -0.000059\n",
      "188797  8.117762e-04 -0.001359\n",
      "188833 -7.648959e-04 -0.000213\n",
      "189043 -5.929274e-04  0.001747\n",
      "189111 -2.983704e-04  0.001491\n",
      "189333  1.466339e-03  0.002145\n",
      "189381 -1.000955e-04 -0.000077\n",
      "189547 -2.916442e-04  0.000784\n",
      "189713 -3.058058e-04 -0.001366\n",
      "190183 -2.081255e-04 -0.000200\n",
      "190207  1.184265e-03 -0.000494\n",
      "190209 -6.109456e-04  0.000320\n",
      "190213  8.916051e-04  0.000388\n",
      "190215 -1.208942e-04  0.002216\n",
      "190219  5.156386e-04  0.000578\n",
      "190221 -1.150334e-03 -0.000845\n",
      "191005  1.010206e-03 -0.002010\n",
      "193565 -1.241513e-03  0.000651\n",
      "193567  5.400139e-04  0.000034\n",
      "193571  3.998800e-04  0.000765\n",
      "193573  8.405586e-04  0.000072\n",
      "193579 -1.365870e-03 -0.002045\n",
      "193581 -1.280914e-03 -0.000956\n",
      "193583 -1.701235e-03  0.000473\n",
      "193585  1.313624e-04 -0.000173\n",
      "193587 -1.187006e-03 -0.000869\n",
      "193609 -1.218491e-04  0.000830\n",
      "\n",
      "[9562 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Code for question 8 here.\n",
    "\n",
    "# 3. To find stochastic gradient\n",
    "\n",
    "iteration = 20\n",
    "q = 2 # the dimension of our map of the 'library'\n",
    "learn_rate = 0.02\n",
    "\n",
    "Y = Y.sample(frac=1).reset_index(drop=True)  # shuffle the dataset\n",
    "U = pd.DataFrame(np.random.normal(size=(nUsersInExample, q))*0.001, index=my_batch_users)\n",
    "V = pd.DataFrame(np.random.normal(size=(n_movies, q))*0.001, index=indexes_unique_movies)\n",
    "nrows = Y.shape[0]\n",
    "\n",
    "for i in range(iteration):  \n",
    "    obj = 0.\n",
    "    for j in range (nrows):\n",
    "        gU = pd.DataFrame(np.zeros((U.shape)), index=U.index)\n",
    "        #print(gU)\n",
    "        gV = pd.DataFrame(np.zeros((V.shape)), index=V.index)\n",
    "        #print(gV)\n",
    "        \n",
    "        row = Y.iloc[i]\n",
    "        user = row['users']\n",
    "        film = row['movies']\n",
    "        rating = row['ratings']\n",
    "        prediction = np.dot(U.loc[user], V.loc[film]) # vTu\n",
    "        diff = prediction - rating # vTu - y\n",
    "        obj += diff*diff\n",
    "        gU.loc[user] += 2*diff*V.loc[film]\n",
    "        gV.loc[film] += 2*diff*U.loc[user]\n",
    "        U -= learn_rate*gU\n",
    "        V -= learn_rate*gV \n",
    "    print(\"After iteration\", i, \"Objective function: \", obj)\n",
    "            \n",
    "print(U)  # final value of U after completing over 20 iterations\n",
    "print(V)  # final value of V after completing over 20 iterations\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x18196c7908>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cd136d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4. To plot the movie map\n",
    "\n",
    "%matplotlib inline\n",
    "import pylab as plt\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "plt.plot(V.iloc[:,0], V.iloc[:,1], 'bx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([1.000e+00, 0.000e+00, 2.000e+00, 3.000e+00, 9.548e+03, 0.000e+00,\n",
       "         1.000e+00, 4.000e+00, 1.000e+00, 2.000e+00]),\n",
       "  array([2.000e+00, 5.000e+00, 1.000e+00, 0.000e+00, 9.547e+03, 2.000e+00,\n",
       "         1.000e+00, 1.000e+00, 3.000e+00, 0.000e+00])],\n",
       " array([-0.99828594, -0.77602232, -0.55375871, -0.3314951 , -0.10923149,\n",
       "         0.11303213,  0.33529574,  0.55755935,  0.77982297,  1.00208658,\n",
       "         1.22435019]),\n",
       " <a list of 2 Lists of Patches objects>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x181940ab70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to get the better insight of data, histogram is plotted. \n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "plt.hist([V.iloc[:,0], V.iloc[:,1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have repeated stochastic gradient algorithm for 20 iterations so that we can reach till convergence. So from the above histogram plot, we can observe that maximum data lies around 0 and has almost reached till convergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
